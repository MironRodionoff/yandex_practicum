# Машинное обучение для текстов
## Определение токсичных комментариев


[md](https://github.com/MironRodionoff/yandex_practicum/blob/main/Project_12/README.md)    
[ipynb](https://github.com/MironRodionoff/yandex_practicum/blob/main/Project_12/Project_12.ipynb)

## Описание проекта

Интернет-магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.
Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.
Построим модель со значением метрики качества F1 не меньше 0.7.
Данные находятся в файле toxic_comments.csv. Столбец text в нём содержит текст комментария, а toxic — целевой признак.


План работы:
* Очистка текста
* Лемматизация
* Токенизация и обучение моделей с помощью pipeline
* Выбор лучшей модели и тестирование

## Навыки и инструменты

- **python**
- **pandas**
- **numpy**
- **nltk**
- **sklearn**

- **catboost**
- **LGBM**  

## Вывод

В ходе исследования были применены следующие методы:

* Токенезация
* Лемматизация
* Очистка текста от символов и бранных слов
* Векторизация для n-грамм. (1 леммa)
Проверены следующие модели:

* Линейная Регрессия
* CatBoostClassifier
* LGBMClassifier

По результатам измерения f1-score рекомендуется использовать модель Логистической Регрессии.
оптимальные параметры: С = 100, class_weight = 'balanced', penalty = 'l2' Результат F1 - 0.78